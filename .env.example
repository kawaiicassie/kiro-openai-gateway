# Kiro OpenAI Gateway - Environment Configuration
# Copy this file to .env and fill in your values

# ===========================================
# REQUIRED
# ===========================================

# Password to protect YOUR proxy server
# This is NOT a token from anywhere - YOU make it up!
# Use this same value as api_key when connecting to your gateway
# Example: "my-super-secret-password-123" or any secure string
PROXY_API_KEY="my-super-secret-password-123"

# ===========================================
# OPTION 1: Kiro IDE credentials (JSON file)
# ===========================================

# Path to JSON credentials file from Kiro IDE
# KIRO_CREDS_FILE="~/.aws/sso/cache/kiro-auth-token.json"

# ===========================================
# OPTION 2: Kiro IDE refresh token
# ===========================================

# Your Kiro refresh token obtained from Kiro IDE traffic.
# REFRESH_TOKEN="your_kiro_refresh_token_here"

# ===========================================
# OPTION 3: JSON credentials as env variable (VPS Deployment)
# ===========================================

# For VPS platforms (Zeabur, Railway, Render, etc.) where you cannot upload files,
# you can pass the entire JSON credentials as an environment variable.
#
# 1. First, set SKIP_ENV_FILE_CHECK=true to bypass .env file check
# 2. Then set KIRO_CREDS_JSON with your credentials JSON content
#
# Example:
# SKIP_ENV_FILE_CHECK=true
# KIRO_CREDS_JSON={"refreshToken":"...","accessToken":"...","profileArn":"...","expiresAt":"..."}

# Skip .env file existence check (set to true for VPS deployment)
# SKIP_ENV_FILE_CHECK=true

# JSON credentials string (copy the entire content of your kiro-auth-token.json)
# KIRO_CREDS_JSON={"refreshToken":"xxx","accessToken":"xxx","profileArn":"arn:aws:...","expiresAt":"2025-12-24T..."}

# ===========================================
# OPTION 4: kiro-cli SQLite database (AWS SSO)
# ===========================================

# Path to kiro-cli SQLite database (for AWS IAM Identity Center users)
# The gateway will auto-detect AWS SSO OIDC and use the correct endpoint
# KIRO_CLI_DB_FILE="~/.local/share/kiro-cli/data.sqlite3"

# ===========================================
# OPTION 5: AWS SSO cache file (kiro-cli)
# ===========================================

# Path to AWS SSO cache file (contains clientId and clientSecret)
# The gateway will auto-detect AWS SSO OIDC and use the correct endpoint
# Note: This uses the same KIRO_CREDS_FILE variable as Option 1
# KIRO_CREDS_FILE="~/.aws/sso/cache/your-sso-cache-file.json"

# ===========================================
# PROFILE ARN (optional)
# ===========================================

# AWS CodeWhisperer profile ARN
# For Kiro IDE: usually auto-detected from credentials file
# For kiro-cli (AWS SSO / Builder ID): not needed, will be ignored
# PROFILE_ARN="arn:aws:codewhisperer:us-east-1:..."

# ===========================================
# OPTIONAL
# ===========================================

# AWS region (default: us-east-1)
# KIRO_REGION="us-east-1"

# ===========================================
# LOGGING
# ===========================================

# Log level: TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO (recommended for production)
# Set to DEBUG for detailed troubleshooting
# LOG_LEVEL="INFO"

# ===========================================
# FIRST TOKEN TIMEOUT (Streaming Retry)
# ===========================================

# Timeout for waiting for the first token from the model (in seconds).
# If the model doesn't respond within this time, the request will be cancelled and retried.
# This helps handle "stuck" requests when the model takes too long to start responding.
# Default: 15 seconds (recommended for production)
# Set a lower value (e.g., 5-10) for more aggressive retry behavior.
# FIRST_TOKEN_TIMEOUT="15"

# Maximum number of retry attempts when first token timeout occurs.
# After exhausting all attempts, a 504 Gateway Timeout error will be returned.
# Default: 3 attempts
# FIRST_TOKEN_MAX_RETRIES="3"

# Read timeout for streaming responses (in seconds).
# This is the maximum time to wait for data between chunks during streaming.
# Should be longer than FIRST_TOKEN_TIMEOUT since the model may pause between chunks
# while "thinking" (especially for tool calls or complex reasoning).
# Default: 300 seconds (5 minutes) - generous timeout to avoid premature disconnects.
# STREAMING_READ_TIMEOUT="300"

# ===========================================
# FAKE REASONING (Extended Thinking via Tag Injection)
# ===========================================

# Enable fake reasoning - injects special tags into requests to enable model reasoning.
# When enabled, the model will include its reasoning process in the response.
# The response is then parsed and converted to OpenAI-compatible reasoning_content format.
#
# WHY "FAKE"? This is NOT native extended thinking API support. Instead, we inject
# <thinking_mode>enabled</thinking_mode> tags into the prompt, and the model responds
# with <thinking>...</thinking> blocks that we parse and convert to reasoning_content.
# It works great, but it's a hack - hence "fake" reasoning.
#
# Default: true (ENABLED by default for premium experience out of the box!)
# To disable, set to false:
# FAKE_REASONING=false

# Maximum thinking length in tokens.
# This value is injected into the request as <max_thinking_length>{value}</max_thinking_length>
# Higher values allow for more detailed reasoning but increase response time and token usage.
# Default: 4000 tokens
# FAKE_REASONING_MAX_TOKENS=4000

# How to handle the thinking block in responses:
# - "as_reasoning_content": Extract to reasoning_content field (OpenAI-compatible, recommended)
# - "remove": Remove thinking block completely, return only final answer
# - "pass": Pass through as-is with original tags in content
# - "strip_tags": Remove tags but keep thinking content in regular content
#
# Default: "as_reasoning_content"
# FAKE_REASONING_HANDLING=as_reasoning_content

# Maximum size of initial buffer for tag detection (characters).
# The parser buffers this many characters before deciding if response contains thinking tags.
# Lower values = faster first token appearance, but may miss tags with leading whitespace.
# Default: 20 characters (enough for longest tag <reasoning> = 11 chars + some whitespace)
# FAKE_REASONING_INITIAL_BUFFER_SIZE=20

# ===========================================
# DEBUG (for development only)
# ===========================================

# Debug logging mode:
# - off: disabled (default)
# - errors: save logs only for failed requests (4xx, 5xx) - recommended for troubleshooting
# - all: save logs for every request (overwrites on each request)
# DEBUG_MODE=off

# Directory for debug log files
# DEBUG_DIR="debug_logs"

# Legacy option (WILL BE REMOVED in future releases, use DEBUG_MODE instead)
# DEBUG_LAST_REQUEST=true is equivalent to DEBUG_MODE=all
# DEBUG_LAST_REQUEST=true
